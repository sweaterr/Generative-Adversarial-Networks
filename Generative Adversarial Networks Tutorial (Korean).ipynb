{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Yann LeCun, “adversarial training is the coolest thing since sliced bread”. I’m inclined to believe so because I don’t think sliced bread ever created this much buzz and excitement within the deep learning community. Generative Adversarial Networks are a set of models that basically learn to create synthetic data that is similar to input data it's given. In more formal terms, a GAN is a generative model that learns the probability distribution (or data distribution) of the training examples it is given. From this distribution, we can then create sample outputs. GANs have seen their largest progress with image training examples, but this idea of modeling data distributions is one that can be applied with other forms of input.In the case described in today’s post, we’ll be creating a GAN that learns to generate synthetic, yet readable, images of MNIST digits. \n",
    "얀 르쿤 (Yann LeCun)에 따르면, \"적대적인 훈련은 얇게 썬 빵 이후 가장 멋진 일\"이라고합니다. 얇게 썬 빵은 깊은 학습 공동체 내에서 이렇게 많은 소란과 흥분을 일으켰다 고 생각하지 않기 때문에 나는 믿을 의향이 있습니다. Generic Adversarial Networks는 주어진 입력 데이터와 유사한 합성 데이터를 생성하는 것을 기본적으로 배우는 일련의 모델입니다. 보다 공식적인 용어로, GAN은 주어진 사례의 확률 분포 (또는 데이터 분포)를 학습하는 생성 모델입니다. 이 분포로부터 샘플 출력을 생성 할 수 있습니다. GAN은 이미지 트레이닝 예제를 통해 가장 큰 진전을 보았지만 데이터 배포 모델링에 대한이 아이디어는 다른 형태의 입력과 함께 적용 할 수있는 아이디어입니다. 오늘의 포스트에서 설명한 경우 합성을 생성하는 것을 배우는 GAN이 생성 될 것입니다 , 아직 읽을 수있는, MNIST 자리의 이미지."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll be explaining generative adversarial networks, and how you can use them to create a generator network that can create realistic MNIST digits through Tensorflow\n",
    "이 노트에서는 생성 적 적자 네트워크에 대해 설명하고 Tensorflow를 통해 현실적인 MNIST 숫자를 생성 할 수있는 발전기 네트워크를 만드는 방법에 대해 설명합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s dig a little bit deeper into the structure of this model. The basic idea of these networks is that you have 2 models, a generative model and a discriminative model. \n",
    "이 모델의 구조를 조금 더 자세히 살펴 보겠습니다. 이 네트워크의 기본 아이디어는 생성 모델과 차별 모델이라는 두 가지 모델을 가지고 있다는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](Images/GAN1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discriminative model has the task of determining whether a given image looks natural (an image from the dataset) or looks like it has been artificially created. This is basically a binary classifier that will take the form of a normal convolutional neural network (CNN).  The task of the generator is to create natural looking images that are similar to the original data distribution. \n",
    "식별 모델은 주어진 이미지가 자연스럽게 보이는지 (데이터 세트의 이미지) 또는 인위적으로 생성 된 것처럼 보이는지를 결정하는 작업을합니다. 이것은 기본적으로 정상 컨볼 루션 신경망 (CNN)의 형태를 취할 이진 분류 자입니다. 생성기의 작업은 원본 데이터 분포와 유사한 자연스러운 이미지를 만드는 것입니다.\n",
    "\n",
    "This can be thought of as a zero-sum or minimax two player game. The analogy used in the paper is that the generative model is like “a team of counterfeiters, trying to produce and use fake currency” while the discriminative model is like “the police, trying to detect the counterfeit currency”. The generator is trying to fool the discriminator while the discriminator is trying to not get fooled by the generator. As the models train through alternating optimization, both methods are improved until a point where the “counterfeits are indistinguishable from the genuine articles”. There are specific game theory concepts that prove there is indeed an equilibrium to this game where the generator gets so good that the discriminator outputs a probability of ½ for every input. \n",
    "이것은 제로섬 또는 미니 맥스 2 인 게임으로 생각할 수 있습니다. 논문에서 사용 된 비유는 생성 모델이 \"가짜 화폐를 생산하고 사용하려고 시도하는 위조자 팀\"인 것과 차별 모델은 \"위조 통화 감지를 시도하는 경찰\"과 같은 것입니다. 판별 기가 발전기에 속지려고 시도하는 동안 발전기가 판별자를 속일려고합니다. 모델이 교대 최적화를 통해 훈련함에 따라 두 방법 모두 \"위조품이 정품과 구별되지 않는\"지점까지 개선됩니다. 이 게임에 대한 평형이 존재한다는 것을 증명하는 특정 게임 이론 개념이 있습니다. 여기서 발전기는 각 입력에 대해 판별자가 ½의 확률을 출력하도록 아주 잘됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we’re going to create a GAN that will generate MNIST digits that can fool even the best classifiers (and humans too of course). Here’s what we’re going to need:\n",
    "\n",
    "- Real MNIST training images\n",
    "- A generator network that takes in a random noise vector and produces a synthetic image\n",
    "- A discriminator network (a CNN) that learns to distinguish between real and synthetic images. You can think of it as just a binary classifier (1 for real image, 0 for fake)\n",
    "- An optimization procedure that jointly updates both networks through SGD. This is the tricky part as we need to train the generator network to fool the discriminator network, which means that we have unique gradient flows and labels. \n",
    "- Tensorflow - Our choice of Deep Learning framework\n",
    "\n",
    "Let’s get started. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading MNIST Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫째, 우리의 import으로 시작합시다. 우리는 주로 Tensorflow가 필요할 것입니다. 또한 Numpy를 가져 와서 일부 매트릭스, 숫자를 생성하는 임의의 라이브러리 및 이미지 데이터를 시각화하는 Matplotlib를 import했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 MNIST 이미지를 가져와야합니다. 이를 위해 read_data_sets라는 TF 함수를 호출합니다. 이것은 MNIST 데이터베이스에 있는 55,000 개의 교육 예제를 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 만든 mnist 변수에는 실제로 이미지와 레이블이 모두 들어 있습니다. 이제 이미지를 분리 해 봅시다. 55,000 개의 이미지가 있고 각 이미지는 28x28 이미지입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = mnist.train.images[:55000,:]\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at what a random image might look like. \n",
    "\n",
    "무작위 이미지가 어떻게 보일지 살펴 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUIQfCIrwA0ERfiAowg8E9f9TjpAKU9De/QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113f49c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "randomNum = random.randint(0,55000)\n",
    "image = x_train[randomNum].reshape([28,28])\n",
    "plt.imshow(image, cmap=plt.get_cmap('gray_r'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Now, using our classical CS concept of modularity, let’s define a CNN classifier function that takes in an image (of size 28 x 28 x 1) as input. The output will be a single scalar number activation that describes whether or not the input image is real or not.\n",
    "   \n",
    "   이제 모듈성이라는 고전적인 CS 개념을 사용하여 이미지 (크기 28 x 28 x 1)를 입력으로 사용하는 CNN 분류 기능을 정의 해 보겠습니다. 출력은 입력 이미지가 실제인지 아닌지를 설명하는 단일 스칼라 번호 활성화입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](Images/GAN2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do that, let's first define some functions that will help us with creating CNNs in Tensorflow\n",
    "\n",
    "먼저 Tensorflow에서 CNN을 만드는 데 도움이되는 몇 가지 기능을 정의 해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(input=x, filter=W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def avg_pool_2x2(x):\n",
    "  return tf.nn.avg_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s see how we’re going to compose this network. We’ll start off by passing the image through a convolutional layer. First, we create our weight and bias variables through tf.get_variable. Our first weight matrix (or filter) will be of size 5x5 and will have a output depth of 8. It will be randomly initialized from a normal distribution.\n",
    "\n",
    "이제 우리가 어떻게 이 네트워크를 구성하는지 봅시다. 우리는 컨벌루션 레이어를 통해 이미지를 전달함으로써 시작할 것입니다. 먼저 tf.get_variable을 통해 가중치 및 바이어스 변수를 만듭니다. 첫 번째 가중치 행렬 (또는 필터)은 크기가 5x5이고 출력 깊이가 8입니다. 정규 분포에서 무작위로 초기화됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we’ll call the function tf.nn.conv2d() through our a helper function called conv2d. tf.nn.conv2d() is the Tensorflow’s function for a common convolution. It takes in 4 arguments. The first is the input volume (our 28 x 28 x 1 image in this case). The next argument is the filter/weight matrix. Finally, you can also change the stride and padding of the convolution. Those two values affect the dimensions of the output volume. \n",
    "\n",
    "그런 다음 conv2d라는 도우미 함수를 통해 tf.nn.conv2d () 함수를 호출합니다. tf.nn.conv2d ()는 일반적인 컨볼 루션을위한 Tensorflow의 기능입니다. 4 개의 인수가 필요합니다. 첫 번째는 입력 볼륨입니다 (이 경우 28x28x1 이미지). 다음 인수는 필터 / 가중치 행렬입니다. 마지막으로 컨볼 루션의 보폭과 패딩을 변경할 수도 있습니다. 이 두 값은 출력 볼륨의 크기에 영향을줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with any convolutional neural network, this module is repeated, and then followed by a series of fully connected layers. At the end of the network, we do a final matrix multiply and return the activation value. For those of you comfortable with CNNs, this is just a simple binary classifier. Nothing fancy. \n",
    "\n",
    "임의의 길쌈 신경 네트워크와 마찬가지로 이 모듈을 반복 한 다음 일련의 완전히 연결된 레이어가 이어집니다. 네트워크가 끝날 때 우리는 마지막 행렬을 곱하고 활성화 값을 반환합니다. CNN에 익숙한 사용자에게는 간단한 바이너리 분류 자입니다. 멋진 일은 없어."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This architecture for this network is based on Tensorflow's sample CNN classifier model that they have explained in detail here: https://www.tensorflow.org/tutorials/mnist/pros/\n",
    "\n",
    "이 네트워크에 대한이 아키텍처는 Tensorflow의 샘플 CNN 분류기 모델 (자세한 내용은 여기에서 설명 함)을 기반으로합니다. https://www.tensorflow.org/tutorials/mnist/pros/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def discriminator(x_image, reuse=False):\n",
    "    if (reuse):\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "    #First Conv and Pool Layers\n",
    "    W_conv1 = tf.get_variable('d_wconv1', [5, 5, 1, 8], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    b_conv1 = tf.get_variable('d_bconv1', [8], initializer=tf.constant_initializer(0))\n",
    "    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "    h_pool1 = avg_pool_2x2(h_conv1)\n",
    "\n",
    "    #Second Conv and Pool Layers\n",
    "    W_conv2 = tf.get_variable('d_wconv2', [5, 5, 8, 16], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    b_conv2 = tf.get_variable('d_bconv2', [16], initializer=tf.constant_initializer(0))\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "    h_pool2 = avg_pool_2x2(h_conv2)\n",
    "\n",
    "    #First Fully Connected Layer\n",
    "    W_fc1 = tf.get_variable('d_wfc1', [7 * 7 * 16, 32], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    b_fc1 = tf.get_variable('d_bfc1', [32], initializer=tf.constant_initializer(0))\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*16])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "    #Second Fully Connected Layer\n",
    "    W_fc2 = tf.get_variable('d_wfc2', [32, 1], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    b_fc2 = tf.get_variable('d_bfc2', [1], initializer=tf.constant_initializer(0))\n",
    "\n",
    "    #Final Layer\n",
    "    y_conv=(tf.matmul(h_fc1, W_fc2) + b_fc2)\n",
    "    return y_conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our discriminator defined, let’s take a look at the generator module. For this, we’ll be basing our model off the generator introduced in the DCGAN paper (link: https://arxiv.org/pdf/1511.06434v2.pdf). You can think of the generator as being a kind of reverse ConvNet. With CNNs, the goal is to transform a 2 or 3 dimensional matrix of pixel values into a single probability. A generator, however, seeks to take a d-dimensional noise vector and upsample it to become a 28 x 28 image. This upsampling is done through a convolutional transpose (or deconvolution) layer. ReLUs and Batch Norm are then used to stabilize the outputs of each layer.\n",
    "\n",
    "discriminator를 정의 했으므로 generator 모듈을 살펴 보겠습니다. 이를 위해 우리는 DCGAN 논문 (https://arxiv.org/pdf/1511.06434v2.pdf 링크)에 소개된 generator를 모델로 삼을 것입니다. generator가 일종의 reverse ConvNet인 것으로 생각할 수 있습니다. CNN을 사용하여 목표는 픽셀 값의 2 또는 3 차원 행렬을 단일 확률로 변환하는 것입니다. 그러나 생성기(generator)는 d 차원 잡음 벡터를 취하여 이를 28 x 28 이미지로 업 샘플링하려고합니다. 이 업 샘플링은 컨벌루션 전치 (convolutional transpose, 또는 deconvolution) 레이어를 통해 수행됩니다. 그런 다음 ReLUs와 Batch Norm을 사용하여 각 레이어의 출력을 안정화합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of the generator is very similar to that of the discriminator, except we're calling the convolution transpose method, instead of the conv2d one. \n",
    "\n",
    "생성자의 구조는 conv2d 대신 convolution transpose 메서드를 호출한다는 점을 제외하고는 판별 자의 구조와 매우 유사합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conv transpose + relu + batch norm pipeline is repeated 4 times so that the output volume grows larger and larger until a 28 x 28 x 1 image is formed.\n",
    "\n",
    "conv transpose + relu + batch norm 파이프 라인을 4 번 반복하여 28 x 28 x 1 이미지가 형성 될 때까지 출력 볼륨이 커집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generator(z, batch_size, z_dim, reuse=False):\n",
    "    if (reuse):\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "    g_dim = 64 #Number of filters of first layer of generator \n",
    "    c_dim = 1 #Color dimension of output (MNIST is grayscale, so c_dim = 1 for us)\n",
    "    s = 28 #Output size of the image\n",
    "    s2, s4, s8, s16 = int(s/2), int(s/4), int(s/8), int(s/16) #We want to slowly upscale the image, so these values will help\n",
    "                                                              #make that change gradual.\n",
    "\n",
    "    h0 = tf.reshape(z, [batch_size, s16+1, s16+1, 25])\n",
    "    h0 = tf.nn.relu(h0)\n",
    "    #Dimensions of h0 = batch_size x 2 x 2 x 25\n",
    "\n",
    "    #First DeConv Layer\n",
    "    output1_shape = [batch_size, s8, s8, g_dim*4]\n",
    "    W_conv1 = tf.get_variable('g_wconv1', [5, 5, output1_shape[-1], int(h0.get_shape()[-1])], \n",
    "                              initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    b_conv1 = tf.get_variable('g_bconv1', [output1_shape[-1]], initializer=tf.constant_initializer(.1))\n",
    "    H_conv1 = tf.nn.conv2d_transpose(h0, W_conv1, output_shape=output1_shape, strides=[1, 2, 2, 1], padding='SAME')\n",
    "    H_conv1 = tf.contrib.layers.batch_norm(inputs = H_conv1, center=True, scale=True, is_training=True, scope=\"g_bn1\")\n",
    "    H_conv1 = tf.nn.relu(H_conv1)\n",
    "    #Dimensions of H_conv1 = batch_size x 3 x 3 x 256\n",
    "\n",
    "    #Second DeConv Layer\n",
    "    output2_shape = [batch_size, s4 - 1, s4 - 1, g_dim*2]\n",
    "    W_conv2 = tf.get_variable('g_wconv2', [5, 5, output2_shape[-1], int(H_conv1.get_shape()[-1])], \n",
    "                              initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    b_conv2 = tf.get_variable('g_bconv2', [output2_shape[-1]], initializer=tf.constant_initializer(.1))\n",
    "    H_conv2 = tf.nn.conv2d_transpose(H_conv1, W_conv2, output_shape=output2_shape, strides=[1, 2, 2, 1], padding='SAME')\n",
    "    H_conv2 = tf.contrib.layers.batch_norm(inputs = H_conv2, center=True, scale=True, is_training=True, scope=\"g_bn2\")\n",
    "    H_conv2 = tf.nn.relu(H_conv2)\n",
    "    #Dimensions of H_conv2 = batch_size x 6 x 6 x 128\n",
    "\n",
    "    #Third DeConv Layer\n",
    "    output3_shape = [batch_size, s2 - 2, s2 - 2, g_dim*1]\n",
    "    W_conv3 = tf.get_variable('g_wconv3', [5, 5, output3_shape[-1], int(H_conv2.get_shape()[-1])], \n",
    "                              initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    b_conv3 = tf.get_variable('g_bconv3', [output3_shape[-1]], initializer=tf.constant_initializer(.1))\n",
    "    H_conv3 = tf.nn.conv2d_transpose(H_conv2, W_conv3, output_shape=output3_shape, strides=[1, 2, 2, 1], padding='SAME')\n",
    "    H_conv3 = tf.contrib.layers.batch_norm(inputs = H_conv3, center=True, scale=True, is_training=True, scope=\"g_bn3\")\n",
    "    H_conv3 = tf.nn.relu(H_conv3)\n",
    "    #Dimensions of H_conv3 = batch_size x 12 x 12 x 64\n",
    "\n",
    "    #Fourth DeConv Layer\n",
    "    output4_shape = [batch_size, s, s, c_dim]\n",
    "    W_conv4 = tf.get_variable('g_wconv4', [5, 5, output4_shape[-1], int(H_conv3.get_shape()[-1])], \n",
    "                              initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    b_conv4 = tf.get_variable('g_bconv4', [output4_shape[-1]], initializer=tf.constant_initializer(.1))\n",
    "    H_conv4 = tf.nn.conv2d_transpose(H_conv3, W_conv4, output_shape=output4_shape, strides=[1, 2, 2, 1], padding='VALID')\n",
    "    H_conv4 = tf.nn.tanh(H_conv4)\n",
    "    #Dimensions of H_conv4 = batch_size x 28 x 28 x 1\n",
    "\n",
    "    return H_conv4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a Sample Image "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자, 이제 우리는 생성기와 discriminator 함수를 정의했습니다. 훈련받지 않은 생성기의 샘플 출력이 어떻게 보이는지 봅시다. Tensorflow를 사용하여 먼저 세션을 정의한 다음 생성자에 대한 입력을 위한 placeholder를 만들어야 합니다. placeholder의 목적은 기본적으로 Tensorflow에 \"나중에 임의의 z 벡터에 입력 할 것입니다. 그러나 지금은 이 placeholder 변수를 대신 정의 할 것입니다.\" Tensorflow는 입력의 크기를 미리 알 수 있습니다. placeholder의 모양은 None x z_dimensions입니다. None 키워드는 세션 런타임에서 값을 판별 할 수 있음을 의미합니다. 가변 일괄 처리 크기를 가질 수 있도록 일반적으로 None을 첫 번째 차원으로 사용합니다 (일괄 처리 크기가 16 인 경우 생성기로의 입력은 16 x 100입니다). None 키워드를 사용하면 나중에 batch_size를 지정할 필요가 없습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "z_dimensions = 100\n",
    "z_test_placeholder = tf.placeholder(tf.float32, [None, z_dimensions])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 생성기의 출력을 보유하는 변수 (sample_image)를 만들고 입력으로 사용할 임의 잡음 벡터를 초기화합니다. np.random.normal 함수는 세 개의 인수를가집니다. 첫 번째와 두 번째는 우리가 원하는 출력 분포의 범위를 정의하고 (우리의 경우 -1와 1 사이), 세 번째는 벡터의 모양 (1 x 100)을 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_image = generator(z_test_placeholder, 1, z_dimensions)\n",
    "test_z = np.random.normal(-1, 1, [1,z_dimensions])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we initialize all the variables, feed our test_z into the placeholder, and run the session. The sess.run function has two arguments. The first is called the \"fetches\" argument. It defines the value for you're interested in computing. For example, in our case, we want to see what the output of the generator is. If you look back at the last code snippet, the output of the generator function is stored in sample_image. Therefore, we'll use sample_image for our first argument. The second argument is where we input our feed_dict. This data structure is where we provide inputs to all of our placeholders. In our example, we need to feed our test_z variable into the z placeholder we defined earlier. \n",
    "\n",
    "다음으로, 모든 변수를 초기화하고, test_z를 플레이스 홀더에 넣고, 세션을 실행합니다. sess.run 기능에는 두 개의 인수가 있습니다. 첫 번째 인수는 \"fetches\"인수라고합니다. 그것은 계산하려는 값를 정의합니다. 예를 들어, 우리의 경우 우리는 생성기의 출력이 무엇인지 보고 싶습니다. 마지막 코드 스니펫을 살펴보면 generator 함수의 출력이 sample_image에 저장됩니다. 따라서 첫 번째 인수에 sample_image를 사용합니다. 두 번째 인수는 feed_dict를 입력하는 곳입니다. 이 데이터 구조는 모든 플레이스홀더에 대한 입력을 제공하는 곳입니다. 이 예에서는 앞에서 정의한 z 플레이스홀더에 test_z 변수를 제공해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "temp = (sess.run(sample_image, feed_dict={z_test_placeholder: test_z}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can view the output through matplotlib. \n",
    "\n",
    "마침내 matplotlib를 통해 출력을 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUWp+kUSp+UUS9X+tk2ODbDSEUgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1148a8b90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_i = temp.squeeze()\n",
    "plt.imshow(my_i, cmap='gray_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a very convincing MNIST digit right? Let’s look at how we can make our generator better. Enter loss functions and optimization!\n",
    "\n",
    "매우 이상한 MNIST 숫자 아닙니까? 생성기를 어떻게 개선 할 수 있는지 보도록 하겠습니다. 손실 함수 및 최적화를 입력하십시오!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "tf.reset_default_graph() #Since we changed our batch size (from 1 to 16), we need to reset our Tensorflow graph\n",
    "\n",
    "sess = tf.Session()\n",
    "x_placeholder = tf.placeholder(\"float\", shape = [None,28,28,1]) #Placeholder for input images to the discriminator\n",
    "z_placeholder = tf.placeholder(tf.float32, [None, z_dimensions]) #Placeholder for input noise vectors to the generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAN을 이해하는 가장 까다로운 부분 중 하나는 손실 기능이 기존의 CNN 분류기보다 약간 복잡하다는 것입니다 (단순 MSE 또는 Hinge Loss가이 트릭을 수행함). 소개로 돌아 가면 GAN은 제로섬 미니 맥스 게임으로 생각할 수 있습니다. 판별 기가 실제 이미지와 생성 된 이미지를 구별하는 데 더 잘 낫도록 노력하는 동안 발전기는 점점 더 사실적인 이미지를 만들기 위해 끊임없이 향상되고 있습니다. 이것은 두 네트워크 모두에 영향을 주는 손실 함수를 공식화해야 함을 의미합니다. 우리 네트워크의 입력과 출력을 살펴 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Dx = discriminator(x_placeholder) #Dx will hold discriminator prediction probabilities for the real MNIST images\n",
    "Gz = generator(z_placeholder, batch_size, z_dimensions) #Gz holds the generated images\n",
    "Dg = discriminator(Gz, reuse=True) #Dg will hold discriminator prediction probabilities for generated images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let’s first think about what we want out of our networks. We want the generator network to create images that will fool the discriminator. The generator wants the discriminator to output a 1 (positive example). Therefore, we want to compute the loss between the Dg and label of 1. This can be done through the tf.nn.sigmoid_cross_entropy_with_logits function. This means that the cross entropy loss will be taken between the two arguments. The \"with_logits\" component means that the function will operate on unscaled values. Basically, this means that instead of using a softmax function to squish the output activations to probability values from 0 to 1, we simply return the unscaled value of the matrix multiplication. Take a look at the last line of our discriminator. There's no softmax or sigmoid layer at the end. \n",
    "\n",
    "먼저 네트워크에서 원하는 것을 생각해 봅시다. 우리는 생성기 네트워크가 판별자를 속일 이미지를 생성하기를 원합니다. 생성기는 판별자가 1을 출력하기를 원합니다 (긍정적 인 예). 따라서 Dg와 레이블 1 사이의 손실을 계산하려고합니다.이 작업은 tf.nn.sigmoid_cross_entropy_with_logits 함수를 통해 수행 할 수 있습니다. 이것은 두 엔티티간에 교차 엔트로피 손실이 발생한다는 것을 의미합니다. \"with_logits\"구성 요소는 스케일되지 않은 값에서 함수가 작동 함을 의미합니다. 기본적으로, 이것은 softmax 함수를 사용하여 출력 액티비티를 0에서 1까지의 확률 값으로 스킬링하는 대신 행렬 곱셈의 스케일되지 않은 값을 반환한다는 것을 의미합니다. 우리의 판별 자의 마지막 줄을보십시오. 마지막에는 softmax 또는 Sigmoid 레이어가 없습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reduce mean function just takes the mean value of all of the components in the matrixx returned by the cross entropy function. This is just a way of reducing the loss to a single scalar value, instead of a vector or matrix. \n",
    "\n",
    "평균 감소 함수는 교차 엔트로피 함수에 의해 반환 된 행렬의 모든 구성 요소의 평균값을 취합니다. 이것은 손실을 벡터 또는 행렬 대신 단일 스칼라 값으로 줄이는 단순한 방법입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Dg, labels=tf.ones_like(Dg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s think about the discriminator’s point of view. Its goal is to just get the correct labels (output 1 for each MNIST digit and 0 for the generated ones). We’d like to compute the loss between Dx and the correct label of 1 as well as the loss between Dg and the correct label of 0. \n",
    "\n",
    "이제 discriminator의 관점에 대해 생각해 봅시다. 그것의 목표는 올바른 라벨을 얻는 것입니다 (각 MNIST 숫자에 대해 1을 출력하고 생성 된 라벨에 0을 출력). 우리는 Dx와 올바른 레이블 1 사이의 손실과 Dg와 올바른 레이블 0 사이의 손실을 계산하려고합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Dx, labels=tf.ones_like(Dx)))\n",
    "d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Dg, labels=tf.zeros_like(Dg)))\n",
    "d_loss = d_loss_real + d_loss_fake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our 2 loss functions (d_loss and g_loss), we need to define our optimizers. Keep in mind that the optimizer for the generator network needs to only update the generator’s weights, not those of the discriminator. In order to make this distinction, we need to create 2 lists, one with the discriminator’s weights and one with the generator’s weights. This is where naming all of your Tensorflow variables can come in handy. \n",
    "\n",
    "2 개의 손실 함수 (d_loss 및 g_loss)를 얻으면 옵티 마이저를 정의해야합니다. 생성기 네트워크 용 옵티 마이저는 판별 자의 가중치가 아닌 생성기의 가중치 만 업데이트하면됩니다. 이 구별을하기 위해 우리는 판별 자의 가중치와 생성기의 가중치가 포함 된 두 개의 목록을 작성해야합니다. 여기서 모든 Tensorflow 변수의 이름을 지정할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tvars = tf.trainable_variables()\n",
    "d_vars = [var for var in tvars if 'd_' in var.name]\n",
    "g_vars = [var for var in tvars if 'g_' in var.name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we specify our two optimizers. In today’s era of deep learning, Adam seems to be the best SGD optimizer as it utilizes adaptive learning rates and momentum. We call Adam's minimize function and also specify the variables that we want it to update. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(tf.get_variable_scope(), reuse=False):\n",
    "    trainerD = tf.train.AdamOptimizer().minimize(d_loss, var_list=d_vars)\n",
    "    trainerG = tf.train.AdamOptimizer().minimize(g_loss, var_list=g_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also specify a learning rate by passing it as an argument (I’ve found .0002 to be effective). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah, now the best part of neural networks, the famous training loop. During every iteration, there will be two updates being made, one to the discriminator and one to the generator. For the generator update, we’ll feed in a random z vector to the generator and pass that output to the discriminator to obtain a probability score (this is the Dg variable we specified earlier). As we remember from our loss function, the cross entropy loss gets minimized, and only the generator’s weights and biases get updated. \n",
    "\n",
    "아, 지금은 신경 네트워크의 가장 중요한 부분, 유명한 훈련 루프. 매 반복마다 두 개의 업데이트가 생성됩니다. 하나는 판별 자로, 다른 하나는 생성기로 업데이트됩니다. 생성기 업데이트의 경우 임의의 z 벡터를 생성기에 공급하고 해당 출력을 판별 자로 전달하여 확률 점수를 얻습니다 (앞에서 지정한 Dg 변수입니다). 우리의 손실 함수에서 기억할 때, 교차 엔트로피 손실은 최소화되고 발전기의 가중치와 편향 만 업데이트됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do the same for the discriminator update. We’ll be taking a batch of images from the mnist variable we created way at the beginning of our program. These will serve as the positive examples, while the images in the previous section are the negative ones.\n",
    "\n",
    "우리는 discriminator 업데이트를 위해 동일한 작업을 수행합니다. 우리는 우리 프로그램의 시작 부분에서 만든 mnist 변수로부터 이미지 배치를 취할 것입니다. 이것들은 긍정적 인 예들로 작용할 것이며, 이전 섹션의 이미지들은 부정적인 것들입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-999b46fe50a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mreal_image_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_image_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrainerD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mz_placeholder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mz_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_placeholder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mreal_image_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Update the discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrainerG\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mz_placeholder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mz_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Update the generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/sweaterr/anaconda/envs/tf0.12/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sweaterr/anaconda/envs/tf0.12/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sweaterr/anaconda/envs/tf0.12/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/sweaterr/anaconda/envs/tf0.12/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sweaterr/anaconda/envs/tf0.12/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "iterations = 3000\n",
    "for i in range(iterations):\n",
    "    z_batch = np.random.normal(-1, 1, size=[batch_size, z_dimensions])\n",
    "    real_image_batch = mnist.train.next_batch(batch_size)\n",
    "    real_image_batch = np.reshape(real_image_batch[0],[batch_size,28,28,1])\n",
    "    _,dLoss = sess.run([trainerD, d_loss],feed_dict={z_placeholder:z_batch,x_placeholder:real_image_batch}) #Update the discriminator\n",
    "    _,gLoss = sess.run([trainerG,g_loss],feed_dict={z_placeholder:z_batch}) #Update the generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see what a sample image looks like after training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x115424590>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1152c7710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_image = generator(z_placeholder, 1, z_dimensions)\n",
    "z_batch = np.random.normal(-1, 1, size=[1, z_dimensions])\n",
    "temp = (sess.run(sample_image, feed_dict={z_placeholder: z_batch}))\n",
    "my_i = temp.squeeze()\n",
    "plt.imshow(my_i, cmap='gray_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Difficulties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One note that I’d like to make is that GANs are notoriously difficult to train. Without the right hyperparameters, network architecture, and training procedure, there is a high chance that either the generator or discriminator will overpower the other. A common case of this is the situation where the generator is able to find a flaw in the discriminator by repeatedly outputting an image that fits the data distribution the discriminator is looking for, but is nowhere close to being a readable MNIST digit. The generator has collapsed onto a single point, and therefore we won’t output a variety of digits. There are also cases where the discriminator becomes too powerful and is able to easily make the distinction between real and fake images. \n",
    "\n",
    "제가 작성하고 싶은 한 가지 메모는 GAN이 훈련하기가 악명 높다는 것입니다. 올바른 하이퍼 파라미터, 네트워크 아키텍처 및 교육 절차가 없으면 생성기 또는 판별 기가 다른 하이퍼 매개 변수를 압도 할 가능성이 높습니다. 이에 대한 일반적인 경우는 판별 기가 찾고있는 데이터 분포에 맞는 이미지를 반복적으로 출력하여 판별 기에서 결함을 찾을 수 있지만 읽을 수있는 MNIST 자리가 거의 없습니다. 발전기가 단일 지점에 접 혔기 때문에 다양한 숫자를 출력하지 않습니다. 판별자가 너무 강력 해져 실제 이미지와 가짜 이미지를 쉽게 구별 할 수있는 경우도 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mathematical intuition behind this phenomenon lies in that GANs are typically trained using gradient descent techniques that are designed to find the minimum value of a cost function, rather than to find the Nash equilibrium of a game. When used to seek for a Nash equilibrium, these algorithms may fail to converge. Further research into game theory and stable optimization techniques may result in GANs that are as easy to train as ConvNets!\n",
    "\n",
    "이 현상의 수학적 직감은 GAN이 일반적으로 게임의 내쉬 균형을 찾는 것보다 비용 함수의 최소값을 찾기 위해 고안된 그래디언트 디센트 기법을 사용하여 훈련된다는 점에 있습니다. 내쉬 평형을 찾는데 사용될 때,이 알고리즘들은 수렴하지 못할 수 있습니다. 게임 이론과 안정적인 최적화 기술에 대한 추가 연구로 ConvNets만큼 쉽게 훈련 할 수 있습니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closing Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post, we showed how two networks were able to play a minimax game in order to capture the data distribution of the MNIST digits and generate similar looking samples. With applications in video frame prediction, text-image mappings, and more, GANs are definitely the hottest topic in deep learning. Hopefully, with this tutorial, you’ve gained a better understanding of how these networks work in practice and how you can build your own with Tensorflow!\n",
    "\n",
    "이 글에서는 두 개의 네트워크가 MNIST 숫자의 데이터 분포를 포착하고 유사한 표본을 생성하기 위해 미니 맥스 게임을 어떻게 할 수 있는지를 보여주었습니다. 비디오 프레임 예측, 텍스트 이미지 매핑 등의 응용 프로그램을 사용하면 GAN이 깊은 학습에서 가장 인기있는 주제입니다. 이 튜토리얼에서는 이러한 네트워크가 실제로 어떻게 작동하는지, Tensorflow로 네트워크를 구축하는 방법에 대해 더 잘 이해했기를 바랍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some more great GAN implementations\n",
    "\n",
    "DCGAN Tensorflow Implentation: https://github.com/carpedm20/DCGAN-tensorflow\n",
    "\n",
    "Arthur Juliani's GAN Implementation: https://github.com/awjuliani/TF-Tutorials/blob/master/DCGAN.ipynb \n",
    "\n",
    "Brandon Amos's Image Completion Project: https://bamos.github.io/2016/08/09/deep-completion/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
